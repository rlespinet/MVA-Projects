\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Training a fully connected neural network}{1}{section.1}}
\newlabel{eq:y-sum}{{{$\star $}}{1}{}{AMS.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Representation of the absolute numeric gradients error of the training point 1 with respect to the parameters at each iteration, for $\xi = 10^{-4}$\relax }}{4}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn-error-gradients-absolute-1-7-50000}{{1}{4}{Representation of the absolute numeric gradients error of the training point 1 with respect to the parameters at each iteration, for $\xi = 10^{-4}$\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Representation of the relative numeric gradients error of the training point 1 with respect to the parameters at each iteration, for $\xi = 10^{-4}$\relax }}{5}{figure.caption.3}}
\newlabel{fig:nn-error-gradients-relative-1-7-50000}{{2}{5}{Representation of the relative numeric gradients error of the training point 1 with respect to the parameters at each iteration, for $\xi = 10^{-4}$\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Representation of the training data point and the decision boundary after $50000$ iterations ($50$ times the number of training points)\relax }}{5}{figure.caption.4}}
\newlabel{fig:nn-contour-7-50000}{{3}{5}{Representation of the training data point and the decision boundary after $50000$ iterations ($50$ times the number of training points)\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of iterations required to reach a given accuracy on validation data\relax }}{5}{table.caption.6}}
\newlabel{tab:validation-iteration-required}{{1}{5}{Number of iterations required to reach a given accuracy on validation data\relax }{table.caption.6}{}}
\newlabel{fig:nn-training-error-7-50000}{{4a}{6}{Training error\relax }{figure.caption.5}{}}
\newlabel{sub@fig:nn-training-error-7-50000}{{a}{6}{Training error\relax }{figure.caption.5}{}}
\newlabel{fig:nn-validation-error-7-50000}{{4b}{6}{Validation error\relax }{figure.caption.5}{}}
\newlabel{sub@fig:nn-validation-error-7-50000}{{b}{6}{Validation error\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representation of the training and validation error as the number of iteration increase\relax }}{6}{figure.caption.5}}
\newlabel{fig:nn-error-7-50000}{{4}{6}{Representation of the training and validation error as the number of iteration increase\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Representation of the sum of the gradient for each of the training points with the number of iterations\relax }}{6}{figure.caption.7}}
\newlabel{fig:nn-gradient-sum-7-50000}{{5}{6}{Representation of the sum of the gradient for each of the training points with the number of iterations\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training and validation error obtained after $50000$ iterations for 5 run of the neural network training with random initializations\relax }}{7}{table.caption.8}}
\newlabel{tab:error-5-try}{{2}{7}{Training and validation error obtained after $50000$ iterations for 5 run of the neural network training with random initializations\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Representation of the training data point and the decision boundary after $50000$ iterations in a case where the network does not converge (only initialization has changed)\relax }}{7}{figure.caption.9}}
\newlabel{fig:nn-contour-div-7-50000}{{6}{7}{Representation of the training data point and the decision boundary after $50000$ iterations in a case where the network does not converge (only initialization has changed)\relax }{figure.caption.9}{}}
\newlabel{fig:nn-training-error-div-7-50000}{{7a}{7}{Training error\relax }{figure.caption.10}{}}
\newlabel{sub@fig:nn-training-error-div-7-50000}{{a}{7}{Training error\relax }{figure.caption.10}{}}
\newlabel{fig:nn-validation-error-div-7-50000}{{7b}{7}{Validation error\relax }{figure.caption.10}{}}
\newlabel{sub@fig:nn-validation-error-div-7-50000}{{b}{7}{Validation error\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Representation of the training and validation error as the number of iteration increase\relax }}{7}{figure.caption.10}}
\newlabel{fig:nn-error-div-7-50000}{{7}{7}{Representation of the training and validation error as the number of iteration increase\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Representation of the sum of the gradient for each of the training points with the number of iterations when the network does not converge\relax }}{8}{figure.caption.11}}
\newlabel{fig:nn-gradient-sum-div-7-50000}{{8}{8}{Representation of the sum of the gradient for each of the training points with the number of iterations when the network does not converge\relax }{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Representation of the training error, validation error and iteration such that next iterations have a validation error below $0.5\%$. (\hspace  {1em}-\hspace  {1em} means that there is no such iteration) (This is obtained after $150000$ iterations with $7$ hidden neurons) \relax }}{8}{table.caption.12}}
\newlabel{tab:learning-rates}{{3}{8}{Representation of the training error, validation error and iteration such that next iterations have a validation error below $0.5\%$. (\hspace {1em}-\hspace {1em} means that there is no such iteration) (This is obtained after $150000$ iterations with $7$ hidden neurons) \relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Representation of the training error, validation error and convergence iteration ($0.5\%$ accuracy on validation data) with different number of hidden neurons. (This is obtained after $50000$ iterations with a learning rate of $0.02$)\relax }}{9}{table.caption.13}}
\newlabel{tab:hidden-neurons}{{4}{9}{Representation of the training error, validation error and convergence iteration ($0.5\%$ accuracy on validation data) with different number of hidden neurons. (This is obtained after $50000$ iterations with a learning rate of $0.02$)\relax }{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Representation of the decision boundary after $50000$ iterations for $h = 100$ hidden units\relax }}{9}{figure.caption.14}}
\newlabel{fig:nn-contour-100-50000}{{9}{9}{Representation of the decision boundary after $50000$ iterations for $h = 100$ hidden units\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}CNN building blocks}{10}{section.2}}
\newlabel{fig:max-pooling-original}{{10a}{11}{Original image\relax }{figure.caption.15}{}}
\newlabel{sub@fig:max-pooling-original}{{a}{11}{Original image\relax }{figure.caption.15}{}}
\newlabel{fig:max-pooling}{{10b}{11}{Result of a max pooling function of size 15\relax }{figure.caption.15}{}}
\newlabel{sub@fig:max-pooling}{{b}{11}{Result of a max pooling function of size 15\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Effect of the max pooling function on an image\relax }}{11}{figure.caption.15}}
\newlabel{fig:max-pooling-effect}{{10}{11}{Effect of the max pooling function on an image\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Difference between the result of the max pooling and the original image\relax }}{11}{figure.caption.16}}
\newlabel{fig:max-pooling-diff}{{11}{11}{Difference between the result of the max pooling and the original image\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Back-propagation and derivatives}{12}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Learning a character CNN}{12}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graphic Representation of the layers of the character convolutional neural network\relax }}{12}{figure.caption.17}}
\newlabel{fig:cnn-characters}{{12}{12}{Graphic Representation of the layers of the character convolutional neural network\relax }{figure.caption.17}{}}
\newlabel{fig:cnn-top1error}{{13a}{14}{Top 1 error\relax }{figure.caption.18}{}}
\newlabel{sub@fig:cnn-top1error}{{a}{14}{Top 1 error\relax }{figure.caption.18}{}}
\newlabel{fig:cnn-top5error}{{13b}{14}{Top 5 error\relax }{figure.caption.18}{}}
\newlabel{sub@fig:cnn-top5error}{{b}{14}{Top 5 error\relax }{figure.caption.18}{}}
\newlabel{fig:cnn-objective}{{13c}{14}{Objective\relax }{figure.caption.18}{}}
\newlabel{sub@fig:cnn-objective}{{c}{14}{Objective\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Representation of the objective, top 1 error and top 5 error with respect to the number of epoch (Done on 15 epoch with batches of 100 images in the stochastic gradient descent\relax }}{14}{figure.caption.18}}
\newlabel{fig:cnn-plots}{{13}{14}{Representation of the objective, top 1 error and top 5 error with respect to the number of epoch (Done on 15 epoch with batches of 100 images in the stochastic gradient descent\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Representation of the $20$ filters of the first convolution layer of the CNN\relax }}{14}{figure.caption.19}}
\newlabel{fig:cnn-filters-layer1}{{14}{14}{Representation of the $20$ filters of the first convolution layer of the CNN\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Graphic Representation of the layers of the character convolutional neural network on an image of size $32 \times (4N+28)$\relax }}{15}{figure.caption.20}}
\newlabel{fig:cnn-characters-N}{{15}{15}{Graphic Representation of the layers of the character convolutional neural network on an image of size $32 \times (4N+28)$\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Input image, and characters (in blue) predicted by the CNN\relax }}{15}{figure.caption.21}}
\newlabel{fig:cnn-prediction}{{16}{15}{Input image, and characters (in blue) predicted by the CNN\relax }{figure.caption.21}{}}
\newlabel{fig:cnn-top1error-jit}{{17a}{16}{Top 1 error\relax }{figure.caption.22}{}}
\newlabel{sub@fig:cnn-top1error-jit}{{a}{16}{Top 1 error\relax }{figure.caption.22}{}}
\newlabel{fig:cnn-top5error-jit}{{17b}{16}{Top 5 error\relax }{figure.caption.22}{}}
\newlabel{sub@fig:cnn-top5error-jit}{{b}{16}{Top 5 error\relax }{figure.caption.22}{}}
\newlabel{fig:cnn-objective-jit}{{17c}{16}{Objective\relax }{figure.caption.22}{}}
\newlabel{sub@fig:cnn-objective-jit}{{c}{16}{Objective\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Representation of the objective, top 1 error and top 5 error with respect to the number of epoch when jittering is applied (Done on 15 epoch with batches of 100 images in the stochastic gradient descent\relax }}{16}{figure.caption.22}}
\newlabel{fig:cnn-plots-jit}{{17}{16}{Representation of the objective, top 1 error and top 5 error with respect to the number of epoch when jittering is applied (Done on 15 epoch with batches of 100 images in the stochastic gradient descent\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Input image, and characters (in blue) predicted by the CNN trained with jittering\relax }}{16}{figure.caption.23}}
\newlabel{fig:cnn-prediction-jit}{{18}{16}{Input image, and characters (in blue) predicted by the CNN trained with jittering\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Using pretrained models}{17}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Image and result of the classification by the CNN\relax }}{17}{figure.caption.24}}
\newlabel{fig:cnn-pepper-top}{{19}{17}{Image and result of the classification by the CNN\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Label and score of the top 5 classes for the image pepper\relax }}{17}{table.caption.25}}
\newlabel{tab:top-5-pepper}{{5}{17}{Label and score of the top 5 classes for the image pepper\relax }{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparison of the features descriptors obtained via a pretrained CNN with features descriptors of assignment 2 for each of the 3 classes (using a SVM classifier for both linear and hellinger kernel)\relax }}{18}{table.caption.26}}
\newlabel{tab:cnn-features}{{6}{18}{Comparison of the features descriptors obtained via a pretrained CNN with features descriptors of assignment 2 for each of the 3 classes (using a SVM classifier for both linear and hellinger kernel)\relax }{table.caption.26}{}}
\newlabel{fig:cnn-features-motorbike-ap}{{20a}{18}{Motorbike class\relax }{figure.caption.27}{}}
\newlabel{sub@fig:cnn-features-motorbike-ap}{{a}{18}{Motorbike class\relax }{figure.caption.27}{}}
\newlabel{fig:cnn-features-airplane-ap}{{20b}{18}{Airplane class\relax }{figure.caption.27}{}}
\newlabel{sub@fig:cnn-features-airplane-ap}{{b}{18}{Airplane class\relax }{figure.caption.27}{}}
\newlabel{fig:cnn-features-person-ap}{{20c}{18}{Person class\relax }{figure.caption.27}{}}
\newlabel{sub@fig:cnn-features-person-ap}{{c}{18}{Person class\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Average precision for each of the 3 classes with CNN based features descriptors and a linear SVM classifier\relax }}{18}{figure.caption.27}}
\newlabel{fig:cnn-features-ap}{{20}{18}{Average precision for each of the 3 classes with CNN based features descriptors and a linear SVM classifier\relax }{figure.caption.27}{}}
