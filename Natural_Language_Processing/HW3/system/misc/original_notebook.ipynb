{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "def html_decode(data):\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    h = HTMLParser()\n",
    "    for line in data:\n",
    "        processed_data.append(h.unescape(line))\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TweetConstructor:\n",
    "    \n",
    "    def __init__(self, max_tweet_len):\n",
    "        self.max_tweet_len = max_tweet_len\n",
    "        \n",
    "    def begin(self):\n",
    "        self.tweets = []\n",
    "        self.cutmarks = []\n",
    "        self.current = []\n",
    "        self.current_len = 0\n",
    "        \n",
    "    def end(self):\n",
    "        self._push_tweet()\n",
    "\n",
    "    def _push_tweet(self, cut=False):\n",
    "        \n",
    "        if self.current_len > 0:\n",
    "            self.tweets.append(self.current)\n",
    "            self.cutmarks.append(cut)\n",
    "            self.current = []\n",
    "            self.current_len = 0\n",
    "            \n",
    "    def _add_current(self, line):\n",
    "        self.current.append(line)\n",
    "        self.current_len += len(line)\n",
    "    \n",
    "    def _clean_line(self, line):\n",
    "        line = line.strip()\n",
    "        \n",
    "        rt = False\n",
    "        cut = False\n",
    "        \n",
    "        if len(line) > 2 and line[:2] == u'RT':\n",
    "            rt = True\n",
    "            line = line[2:]\n",
    "            \n",
    "        if len(line) > 1 and line[-1] == u'\\u2026':\n",
    "            cut = True\n",
    "            line = line[:-1]\n",
    "            \n",
    "        line = line.strip()\n",
    "        return (line, rt, cut)\n",
    "    \n",
    "    \n",
    "    def add_line(self, line):\n",
    "        \n",
    "        line, rt, cut = self._clean_line(line)\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if rt:\n",
    "            self._push_tweet()\n",
    "        \n",
    "        self._add_current(line)\n",
    "        \n",
    "        if cut:\n",
    "            self._push_tweet(cut=True)\n",
    "                \n",
    "def reconstruct_tweets(data):\n",
    "    tc = TweetConstructor(140)\n",
    "\n",
    "    tc.begin()\n",
    "    \n",
    "    for line in data:\n",
    "        tc.add_line(line)\n",
    "        \n",
    "    tc.end()\n",
    "    \n",
    "    return tc.tweets, tc.cutmarks\n",
    "\n",
    "def merge_tweets(tweets):\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "        data.append((u\"\\n\").join(tweet))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Merge the tweets based on a heuristic (dynamic programming approach)\n",
    "def merge_tweets_dp(tweets):\n",
    "    \n",
    "    max_len = 141\n",
    "    merged_tweets = []\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        W = np.zeros((len(tweet), len(tweet)), dtype=float)\n",
    "        I = -np.ones((len(tweet), len(tweet)), dtype=int)\n",
    "        L = np.array([len(w) for w in tweet]) + 1\n",
    "        C = np.hstack((0, np.cumsum(L)))\n",
    "        C[-1] -= 1\n",
    "        \n",
    "        n = np.ceil(float(C[-1]) / max_len)\n",
    "                \n",
    "        for d in range(0, len(tweet)):\n",
    "            for i in range(len(tweet) - d):\n",
    "                if C[i + d + 1] - C[i] <= max_len:\n",
    "                    W[i, i + d] = np.abs(C[-1] / n - (C[i + d + 1] - C[i]))\n",
    "                else:\n",
    "                    I[i, i + d] = np.argmin(W[i, i:(i+d)] + W[(i+1):(i+d+1), i + d]) + 1\n",
    "                    W[i, i + d] = np.min(W[i, i:(i+d)] + W[(i+1):(i+d+1), i + d])\n",
    "        \n",
    "        merged_tweet = []\n",
    "        d = 0\n",
    "        while True:\n",
    "            i = I[d, -1]\n",
    "            if i == -1:\n",
    "                break\n",
    "            d += i\n",
    "            merged_tweet.append(u'\\n'.join(tweet[:i]))\n",
    "            tweet = tweet[i:]\n",
    "        merged_tweet.append(u'\\n'.join(tweet))\n",
    "        \n",
    "        merged_tweets.extend(merged_tweet)\n",
    "    \n",
    "    return merged_tweets\n",
    "\n",
    "def remove_cut_words(tweets, cutmarks):\n",
    "    cleaned_tweets = []\n",
    "    for tweet, cut in zip(tweets, cutmarks):\n",
    "        if cut:\n",
    "            id = tweet.rfind(' ')\n",
    "            tweet = tweet[:id]\n",
    "        cleaned_tweets.append(tweet)\n",
    "    return cleaned_tweets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def link_hook(string):\n",
    "    return \"<LINK:\" + string.group(0) + \">\"\n",
    "\n",
    "def emoji_hook(string):         \n",
    "    return '<EMOJIS:' + string.group().encode('unicode-escape') + '>'\n",
    "\n",
    "def name_hook(string):         \n",
    "    return '<NAME:' + string.group(1) + '>'\n",
    "\n",
    "def hashtag_hook(string):         \n",
    "    return '<HASH:' + string + '>'\n",
    "\n",
    "def process_links(lines, remove=False):\n",
    "    \n",
    "    process = '' if remove else link_hook\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = re.sub('(https?://\\S*)', process, line)\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "    return cleaned_lines\n",
    "\n",
    "def process_names(lines, remove=False):\n",
    "    \n",
    "    process = '' if remove else name_hook\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = re.sub('@(\\w+):?', process, line)\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "    return cleaned_lines\n",
    "\n",
    "\n",
    "def process_hashtags(lines, remove=False, heuristic=True):\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        words = re.split('(#\\w+)', line)\n",
    "        for i, word in enumerate(words):\n",
    "            if len(word) == 0 or word[0] != '#':\n",
    "                continue\n",
    "            \n",
    "            if heuristic and re.match(\"^#(([A-Z]+)|([a-zA-Z][a-z]+))$\", word):\n",
    "                words[i] = word[1:]\n",
    "            elif remove:\n",
    "                words[i] = ''\n",
    "            else:\n",
    "                hashtag_hook(word)\n",
    "\n",
    "        cleaned_lines.append(''.join(words))\n",
    "    return cleaned_lines\n",
    "\n",
    "def process_emojis(lines, remove=False):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    process = '' if remove else emoji_hook\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = emoji_pattern.sub(process, line)\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "    return cleaned_lines\n",
    "\n",
    "def case_hook(string):\n",
    "    return string.group(0) if len(string.group(0)) == 1 else string.group(0).lower()\n",
    "\n",
    "def process_case(lines):\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = re.sub(r\"[A-Z']+\", case_hook, line, re.UNICODE)\n",
    "        cleaned_lines.append(line)\n",
    "    return cleaned_lines\n",
    "\n",
    "def process_case(lines):\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_lines.append(line.lower())\n",
    "    return cleaned_lines\n",
    "\n",
    "def process_contractions(lines):\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # From stackoverflow\n",
    "        # https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/19794953#19794953\n",
    "        line = re.sub(r\"won't\", \"will not\", line, re.UNICODE)\n",
    "        line = re.sub(r\"can\\'t\", \"can not\", line, re.UNICODE)\n",
    "\n",
    "        # general\n",
    "        line = re.sub(r\"\\'ve\", \" have\", line, re.UNICODE)\n",
    "        line = re.sub(r\"n\\'t\", \" not\", line, re.UNICODE)\n",
    "        line = re.sub(r\"\\'re\", \" are\", line, re.UNICODE)\n",
    "#         line = re.sub(r\"\\'s\", \" is\", line, re.UNICODE) # This one is \n",
    "        line = re.sub(r\"\\'d\", \" would\", line, re.UNICODE)\n",
    "        line = re.sub(r\"\\'ll\", \" will\", line, re.UNICODE)\n",
    "        line = re.sub(r\"\\'t\", \" not\", line, re.UNICODE)\n",
    "        line = re.sub(r\"\\'m\", \" am\", line, re.UNICODE)\n",
    "        cleaned_lines.append(line)\n",
    "    return cleaned_lines\n",
    "\n",
    "def clean_non_ascii(lines):\n",
    "    \n",
    "    contraction_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = line.encode('ascii',errors='ignore')\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "    return cleaned_lines\n",
    "\n",
    "import csv\n",
    "\n",
    "def normalization_dictionaries(lines, dictionaries):\n",
    "\n",
    "    token_lines = []\n",
    "    for line in lines:\n",
    "        token_lines.append(re.split(\"([\\w']+)\", line))\n",
    "    \n",
    "    for dictionary in dictionaries:\n",
    "        dic = {}\n",
    "        with open(dictionary, 'r') as file:\n",
    "            reader = csv.reader(file, delimiter='|')\n",
    "            for row in reader:\n",
    "                dic[row[0].strip()] = row[1].strip()\n",
    "\n",
    "        for tokens in token_lines:\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token in dic:\n",
    "                    tokens[i] = dic[token]\n",
    "    #                 print token\n",
    "\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for tokens in token_lines:\n",
    "        cleaned_lines.append(''.join(tokens))\n",
    "\n",
    "    return cleaned_lines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = None\n",
    "        self.letter = None\n",
    "\n",
    "# Constructs a greedy tree\n",
    "def construct_tree_rec(U, ids, alphamap, words):\n",
    "    if U.shape[0] < 20 or U.shape[1] <= 1:\n",
    "        T = Tree()\n",
    "        T.data = ids\n",
    "        return T\n",
    "    \n",
    "    bestd = np.inf\n",
    "    bests = 0\n",
    "    besti = 0\n",
    "    \n",
    "    for i, u in enumerate(U.T):\n",
    "        s = int(np.sum(u))\n",
    "        d = np.abs(len(U) / 2 - s)\n",
    "        if (d < bestd):\n",
    "            bests = s\n",
    "            bestd = d\n",
    "            besti = i\n",
    "\n",
    "    T = Tree()\n",
    "    T.letter = alphamap[besti]          \n",
    "\n",
    "    reorder = np.argsort(U.T[besti])\n",
    "    sep = len(U) - bests\n",
    "    \n",
    "    alphamap = np.delete(alphamap, besti)\n",
    "    U = np.delete(U[reorder], besti, axis=1)\n",
    "    ids = ids[reorder]\n",
    "    \n",
    "    Ul = U[:sep]\n",
    "    Ur = U[sep:]\n",
    "    idsl = ids[:sep]\n",
    "    idsr = ids[sep:]\n",
    "\n",
    "    if (len(idsl) > 0): T.left = construct_tree_rec(Ul, idsl, alphamap, words)\n",
    "    if (len(idsr) > 0): T.right = construct_tree_rec(Ur, idsr, alphamap, words)\n",
    "    return T\n",
    "\n",
    "\n",
    "def candidate_edit_dist(T, word, edit=2):\n",
    "    \n",
    "    if edit < 0:\n",
    "        return np.array([], dtype=int)\n",
    "    \n",
    "    if T.left is None or T.right is None:\n",
    "        return T.data if T.data is not None else np.array([], dtype=int)\n",
    "    \n",
    "    if T.letter in word:\n",
    "        left = candidate_edit_dist(T.left, word, edit-1)\n",
    "        right = candidate_edit_dist(T.right, word, edit)\n",
    "    else:\n",
    "        left = candidate_edit_dist(T.left, word, edit)\n",
    "        right = candidate_edit_dist(T.right, word, edit-1)\n",
    "    \n",
    "    return np.hstack((left, right))\n",
    "\n",
    "def candidate_edit_dist(T, word, edit=2):\n",
    "    \n",
    "    if edit < 0:\n",
    "        return np.array([], dtype=int)\n",
    "    \n",
    "    if T.left is None and T.right is None:\n",
    "        return T.data if T.data is not None else np.array([], dtype=int)\n",
    "    \n",
    "    if T.left is None:\n",
    "        left = np.array([], dtype=int)\n",
    "    elif T.letter in word:\n",
    "        left = candidate_edit_dist(T.left, word, edit-1)\n",
    "    else:\n",
    "        left = candidate_edit_dist(T.left, word, edit)\n",
    "        \n",
    "    if T.right is None:\n",
    "        right = np.array([], dtype=int)\n",
    "    elif T.letter in word:\n",
    "        right = candidate_edit_dist(T.right, word, edit)\n",
    "    else:\n",
    "        right = candidate_edit_dist(T.right, word, edit-1)\n",
    "    \n",
    "    return np.hstack((left, right))\n",
    "\n",
    "def clean_dictionary(words):\n",
    "    \n",
    "    # Clean dictionary\n",
    "    cleaned_words = []\n",
    "    \n",
    "    dic = {}\n",
    "    for word in nltk.corpus.words.words():\n",
    "        dic[word] = 1\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        word = word.lower()\n",
    "        if re.match('^\\w+$', word) is None:\n",
    "            continue\n",
    "            \n",
    "        if word not in dic:\n",
    "            continue\n",
    "            \n",
    "        cleaned_words.append(word)\n",
    "            \n",
    "    print(\"removed : %d\" % (len(words) - len(cleaned_words)))\n",
    "\n",
    "    return np.array(cleaned_words)\n",
    "\n",
    "def construct_tree(cleaned_words):\n",
    "    \n",
    "    cleaned_words = np.array(cleaned_words)\n",
    "\n",
    "    alphabet = {key: 1 for key in ''.join(cleaned_words)}\n",
    "    alphamap = np.empty(len(alphabet), dtype='|S1')\n",
    "    for i, key in enumerate(alphabet.keys()):\n",
    "        alphabet[key] = i\n",
    "        alphamap[i] = key\n",
    "    \n",
    "    U = np.empty((len(cleaned_words), len(alphabet)))\n",
    "    for id, word in enumerate(cleaned_words):\n",
    "        vec = np.zeros(len(alphabet))\n",
    "        for l in word:\n",
    "            vec[alphabet[l]] = 1\n",
    "        \n",
    "        U[id] = vec\n",
    "    \n",
    "    ids = np.arange(len(cleaned_words), dtype=int)\n",
    "    T = construct_tree_rec(U, ids, alphamap, cleaned_words)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This algorithm has been adapted from the pseudo-code given\n",
    "# by https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "class Levenshtein:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.m = 1\n",
    "        self.n = 1\n",
    "        self.init_dist()\n",
    "    \n",
    "    def init_dist(self):\n",
    "        self.d = np.empty((self.m + 1, self.n + 1), dtype=int)\n",
    "        self.d[0, 0] = 0\n",
    "        self.d[1:, 0] = np.arange(self.m) + 1\n",
    "        self.d[0, 1:] = np.arange(self.n) + 1\n",
    "    \n",
    "    def dist(self, s, t):\n",
    "    \n",
    "        m, n = len(s), len(t)\n",
    "    \n",
    "        # Reallocate and initialize only if necessary\n",
    "        if m > self.m or n > self.n:\n",
    "            self.m = max(self.m, m)\n",
    "            self.n = max(self.n, n)\n",
    "            self.init_dist()\n",
    "\n",
    "        for j in range(n):\n",
    "            for i in range(m):\n",
    "                cost = 0 if t[j] == s[i] else 1\n",
    "                self.d[i+1, j+1] = min(self.d[i, j+1] + 1,  # deletion\n",
    "                                       self.d[i+1, j] + 1,  # insertion\n",
    "                                       self.d[i, j] + cost) # substitution\n",
    "        return self.d[m, n]\n",
    "    \n",
    "class DamerauLevenshtein:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.m = 1\n",
    "        self.n = 1\n",
    "        self.init_dist()\n",
    "    \n",
    "    def init_dist(self):\n",
    "        self.d = np.empty((self.m + 1, self.n + 1), dtype=int)\n",
    "        self.d[0, 0] = 0\n",
    "        self.d[1:, 0] = np.arange(self.m) + 1\n",
    "        self.d[0, 1:] = np.arange(self.n) + 1\n",
    "    \n",
    "    def dist(self, s, t):\n",
    "    \n",
    "        m, n = len(s), len(t)\n",
    "    \n",
    "        # Reallocate and initialize only if necessary\n",
    "        if m > self.m or n > self.n:\n",
    "            self.m = max(self.m, m)\n",
    "            self.n = max(self.n, n)\n",
    "            self.init_dist()\n",
    "\n",
    "        for j in range(n):\n",
    "            for i in range(m):\n",
    "                cost = 0 if t[j] == s[i] else 1\n",
    "                self.d[i+1, j+1] = min(self.d[i, j+1] + 1,  # deletion\n",
    "                                       self.d[i+1, j] + 1,  # insertion\n",
    "                                       self.d[i, j] + cost) # substitution\n",
    "                if i > 0 and j > 0 and s[i] == t[j-1] and s[i-1] == t[j]:\n",
    "                    self.d[i+1, j+1] = min(self.d[i+1, j+1],\n",
    "                                           self.d[i-1, j-1] + cost)\n",
    "        return self.d[m, n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize(tweets):\n",
    "    token_tweets = []\n",
    "    for tweet in tweets:\n",
    "        token_tweets.append(nltk.word_tokenize(tweet))\n",
    "    return token_tweets\n",
    "\n",
    "def pos_tagging(token_tweets):\n",
    "    tags = []\n",
    "    for tokens in token_tweets:\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        tags.append([t[1] for t in tagged])\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_context_similarity(token_tweets, tweets_tags):\n",
    "\n",
    "    edit_dist = DamerauLevenshtein()\n",
    "\n",
    "    aff_edit = [1, 0.3, 0.1]\n",
    "\n",
    "    tokens_list = []\n",
    "\n",
    "    for token_list, tag_list in zip(token_tweets, tweets_tags):\n",
    "        processed_tokens = []\n",
    "        for j, (word, tag) in enumerate(zip(token_list, tag_list)):\n",
    "\n",
    "            word = word.lower()\n",
    "\n",
    "            if tag == 'NNP' or len(tag) < 2:\n",
    "                processed_tokens.append(word)\n",
    "                continue\n",
    "\n",
    "\n",
    "            contextv = model.context2vec(token_list, j)\n",
    "            contextv = np.dot(w, contextv)\n",
    "\n",
    "            ids = candidate_edit_dist(T, word)\n",
    "            candidates = cleaned_words[ids]\n",
    "\n",
    "\n",
    "            best_aff = 0\n",
    "            best_candidate = ''\n",
    "            for candidate in candidates:\n",
    "\n",
    "                ed = edit_dist.dist(word, candidate)\n",
    "                if ed > 2: continue\n",
    "\n",
    "                target = aff_edit[ed]\n",
    "                context = contextv[word2index[candidate]]\n",
    "\n",
    "                aff = context * target\n",
    "                if aff > best_aff:\n",
    "                    best_aff = aff\n",
    "                    best_candidate = candidate           \n",
    "\n",
    "            if best_aff == 0 or best_aff < 0.1:\n",
    "                best_candidate = word\n",
    "    #         else:\n",
    "    #             print word + \" -> \" + best_candidate + \" \" + str(best_aff)\n",
    "\n",
    "            processed_tokens.append(best_candidate)\n",
    "\n",
    "        tokens_list.append(processed_tokens)\n",
    "\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: data/context2vec.ukwac.model.params\n",
      "Config:  {'config_path': 'data/', 'model_file': 'context2vec.ukwac.model', 'deep': 'yes', 'drop_ratio': '0.0', 'words_file': 'context2vec.ukwac.words.targets', 'unit': '300'}\n"
     ]
    }
   ],
   "source": [
    "from context2vec.common.model_reader import ModelReader\n",
    "\n",
    "model_param_file = \"data/context2vec.ukwac.model.params\"\n",
    "\n",
    "model_reader = ModelReader(model_param_file)\n",
    "w = model_reader.w\n",
    "word2index = model_reader.word2index\n",
    "index2word = model_reader.index2word\n",
    "model = model_reader.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160562/160562 [00:00<00:00, 780787.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed : 122297\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = clean_dictionary(model_reader.index2word)\n",
    "T = construct_tree(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = [\"@RT_com: #Paris txi drivers turned off their meters, took people home for free - reports https://t.co/UUjfMTCXsM https://t.co/1TicKMbNJy\",\n",
    "         \"I've no idea were you could find this\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_before = tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = process_links(tweets, remove=True)\n",
    "tweets = process_emojis(tweets, remove=True)\n",
    "tweets = process_names(tweets, remove=True)\n",
    "tweets = process_hashtags(tweets, remove=True)\n",
    "tweets = clean_non_ascii(tweets)\n",
    "tweets = process_case(tweets)\n",
    "# Adapted from http://www.smart-words.org/abbreviations/text.html\n",
    "# and http://www.hlt.utdallas.edu/~yangl/data/Text_Norm_Data_Release_Fei_Liu/\n",
    "tweets = normalization_dictionaries(tweets, ['data/Test_Set_3802_Pairs.txt', 'data/short_abbrev_list.txt'])\n",
    "tweets = process_contractions(tweets)\n",
    "\n",
    "token_tweets = tokenize(tweets)\n",
    "tweets_tags = pos_tagging(token_tweets)\n",
    "\n",
    "final_tokens = process_context_similarity(token_tweets, tweets_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@RT_com: #Paris txi drivers turned off their meters, took people home for free - reports https://t.co/UUjfMTCXsM https://t.co/1TicKMbNJy\n",
      "paris taxi drivers turned off their meters , took people home for free - reports\n",
      "---\n",
      "I've no idea were you could find this\n",
      "i have no idea where you could find this\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(final_tokens, tweets_before):\n",
    "    print(b)\n",
    "    print(' '.join(a))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TD3_NLP",
   "language": "python",
   "name": "td3_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
